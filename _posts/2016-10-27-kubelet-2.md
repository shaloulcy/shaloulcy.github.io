---
layout: post 
title:  "kubelet源码解析(2)"
categories: k8s, kubelet
---

*本源码分析基于k8s v1.3.6*

在kubelet源码解析(2)中，我们解析了kubelet是如何获取Pod信息的，以及如何处理Pod的。在这一章节中，我们将介绍kubelet中几种重要的manager。

## 一、volumeManager


### 1、volumeManager的创建

volumeManager的创建发生在创建Kubelet时，函数NewMainKubelet用于创建kubelet。里面调用

```
klet.volumeManager, err = volumemanager.NewVolumeManager(
		enableControllerAttachDetach,
		nodeName,
		klet.podManager,
		klet.kubeClient,
		klet.volumePluginMgr,
		klet.containerRuntime,
		klet.getPodsDir())
```

展开NewVolumeManager函数。

**kubernetes/pkg/kubelet/volumemanager/volume_manager.go +142**

```
func NewVolumeManager(
	controllerAttachDetachEnabled bool,
	hostName string,
	podManager pod.Manager,
	kubeClient internalclientset.Interface,
	volumePluginMgr *volume.VolumePluginMgr,
	kubeContainerRuntime kubecontainer.Runtime,
	kubeletPodsDir string) (VolumeManager, error) {
	vm := &volumeManager{
		kubeClient:          kubeClient,
		volumePluginMgr:     volumePluginMgr,
		desiredStateOfWorld: cache.NewDesiredStateOfWorld(volumePluginMgr),
		actualStateOfWorld:  cache.NewActualStateOfWorld(hostName, volumePluginMgr),
		operationExecutor: operationexecutor.NewOperationExecutor(
			kubeClient,
			volumePluginMgr),
	}

	vm.reconciler = reconciler.NewReconciler(
		kubeClient,
		controllerAttachDetachEnabled,
		reconcilerLoopSleepPeriod,
		reconcilerReconstructSleepPeriod,
		waitForAttachTimeout,
		hostName,
		vm.desiredStateOfWorld,
		vm.actualStateOfWorld,
		vm.operationExecutor,
		volumePluginMgr,
		kubeletPodsDir)
	vm.desiredStateOfWorldPopulator = populator.NewDesiredStateOfWorldPopulator(
		kubeClient,
		desiredStateOfWorldPopulatorLoopSleepPeriod,
		desiredStateOfWorldPopulatorGetPodStatusRetryDuration,
		podManager,
		vm.desiredStateOfWorld,
		kubeContainerRuntime)

	return vm, nil
}
```

几个重要的结构成员volumePluginMgr、desiredStateOfWorld、actualStateOfWorld、reconciler、desiredStateOfWorldPopulator


### 2、volumeManager的运行

主要启动了两个go routine，一个执行vm.desiredStateOfWorldPopulator.Run(stopCh)，另一个执行vm.reconciler.Run(sourcesReady, stopCh)。想要知道这几个go routine干什么，首先需要理解desiredStateOfWorld和actualStateOfWorld

**kubernetes/pkg/kubelet/volumemanager/volume_manager.go +221**

```
func (vm *volumeManager) Run(sourcesReady config.SourcesReady, stopCh <-chan struct{}) {
	defer runtime.HandleCrash()

	go vm.desiredStateOfWorldPopulator.Run(stopCh)
	glog.V(2).Infof("The desired_state_of_world populator starts")

	glog.Infof("Starting Kubelet Volume Manager")
	go vm.reconciler.Run(sourcesReady, stopCh)

	<-stopCh
	glog.Infof("Shutting down Kubelet Volume Manager")
}
```

### 3、desiredStateOfWorld和actualStateOfWorld

desiredStateOfWorld按照单词的意思，可以理解为理想的volume情况，它主要是根据podManger获取所有的Pod信息，从中提取Volume信息。

而actualStateOfWorld则是实际的volume情况。

desiredStateOfWorldPopulator通过podManager去构建desiredStateOfWorld。

而reconciler的工作主要是比较actualStateOfWorld和desiredStateOfWorld的差别，然后进行volume的创建、删除和修改，最后使二者达到一致。


### 4、desiredStateOfWorldPopulator.Run(stopCh)

展开该函数

**kubernetes/pkg/kubelet/volumemanager/populator/desired_state_of_world_populator.go +101**

```go
func (dswp *desiredStateOfWorldPopulator) Run(stopCh <-chan struct{}) {
	wait.Until(dswp.populatorLoopFunc(), dswp.loopSleepDuration, stopCh)
}

func (dswp *desiredStateOfWorldPopulator) populatorLoopFunc() func() {
	return func() {
		dswp.findAndAddNewPods()

		if time.Since(dswp.timeOfLastGetPodStatus) < dswp.getPodStatusRetryDuration {
			glog.V(5).Infof(
				"Skipping findAndRemoveDeletedPods(). Not permitted until %v (getPodStatusRetryDuration %v).",
				dswp.timeOfLastGetPodStatus.Add(dswp.getPodStatusRetryDuration),
				dswp.getPodStatusRetryDuration)

			return
		}

		dswp.findAndRemoveDeletedPods()
	}
}
```

首先执行findAndAddNewPods，然后执行findAndRemoveDeletedPods，由于findAndRemoveDeletedPods代价比较高昂，因此会检查执行的间隔时间

findAndAddNewPods的逻辑比较简单。就是通过podManager获取所有的pods，然后调用processPodVolumes去更新desiredStateOfWorld。但是这样只能更新新增加的Pods的Volume信息。


```go
func (dswp *desiredStateOfWorldPopulator) findAndAddNewPods() {
	for _, pod := range dswp.podManager.GetPods() {
		dswp.processPodVolumes(pod)
	}
}
```

findAndRemoveDeletedPods会遍历所有的volumeToMount，首先会判断该Volume所属的Pod是否存在于podManager，假如存在则忽略，说明不需要删除。

假如不存在，调用dswp.kubeContainerRuntime.GetPods(false)抓取Pod信息，这里是调用kubeContainerRuntime的GetPods函数。因此获取的都是runningPods信息，即正在运行的Pod信息。由于一个volume可以属于多个Pod，而一个Pod可以包含多个container，每个container都可以使用volume，所以他要扫描该volume所属的Pod的container信息，确保没有container使用该volume，才会删除该volume



```go
func (dswp *desiredStateOfWorldPopulator) findAndRemoveDeletedPods() {
	var runningPods []*kubecontainer.Pod

	runningPodsFetched := false
	for _, volumeToMount := range dswp.desiredStateOfWorld.GetVolumesToMount() {
		if _, podExists :=
			dswp.podManager.GetPodByUID(volumeToMount.Pod.UID); podExists {
			continue
		}

		if !runningPodsFetched {
			var getPodsErr error
			runningPods, getPodsErr = dswp.kubeContainerRuntime.GetPods(false)
			if getPodsErr != nil {
				glog.Errorf(
					"kubeContainerRuntime.findAndRemoveDeletedPods returned error %v.",
					getPodsErr)
				continue
			}

			runningPodsFetched = true
			dswp.timeOfLastGetPodStatus = time.Now()
		}

		runningContainers := false
		for _, runningPod := range runningPods {
			if runningPod.ID == volumeToMount.Pod.UID {
				if len(runningPod.Containers) > 0 {
					runningContainers = true
				}

				break
			}
		}

		if runningContainers {
			glog.V(5).Infof(
				"Pod %q has been removed from pod manager. However, it still has one or more containers in the non-exited state. Therefore it will not be removed from volume manager.",
				format.Pod(volumeToMount.Pod))
			continue
		}

		glog.V(5).Infof(
			"Removing volume %q (volSpec=%q) for pod %q from desired state.",
			volumeToMount.VolumeName,
			volumeToMount.VolumeSpec.Name(),
			format.Pod(volumeToMount.Pod))

		dswp.desiredStateOfWorld.DeletePodFromVolume(
			volumeToMount.PodName, volumeToMount.VolumeName)
		dswp.deleteProcessedPod(volumeToMount.PodName)
	}
}
```

通过以上两步，desiredStateOfWorld就构建出来了，这是理想的volume状态，这里并没有发生实际的volume的创建删除挂载卸载操作。实际的操作由reconciler.Run(sourcesReady, stopCh)完成

### 5、reconciler.Run(sourcesReady, stopCh)

```go
func (rc *reconciler) Run(sourcesReady config.SourcesReady, stopCh <-chan struct{}) {
	wait.Until(rc.reconciliationLoopFunc(sourcesReady), rc.loopSleepDuration, stopCh)
}

func (rc *reconciler) reconciliationLoopFunc(sourcesReady config.SourcesReady) func() {
	return func() {
		rc.reconcile()

		if sourcesReady.AllReady() && time.Since(rc.timeOfLastReconstruct) > rc.reconstructDuration {
			glog.V(5).Infof("Sources are all ready, starting reconstruct state function")
			rc.reconstruct()
		}
	}
}

func (rc *reconciler) reconcile() {
	for _, mountedVolume := range rc.actualStateOfWorld.GetMountedVolumes() {
		if !rc.desiredStateOfWorld.PodExistsInVolume(mountedVolume.PodName, mountedVolume.VolumeName) {
			err := rc.operationExecutor.UnmountVolume(
				mountedVolume.MountedVolume, rc.actualStateOfWorld)
		}
	}

	for _, volumeToMount := range rc.desiredStateOfWorld.GetVolumesToMount() {
		volMounted, devicePath, err := rc.actualStateOfWorld.PodExistsInVolume(volumeToMount.PodName, volumeToMount.VolumeName)
		volumeToMount.DevicePath = devicePath
		if cache.IsVolumeNotAttachedError(err) {
			if rc.controllerAttachDetachEnabled || !volumeToMount.PluginIsAttachable {
				err := rc.operationExecutor.VerifyControllerAttachedVolume(
					volumeToMount.VolumeToMount,
					rc.hostName,
					rc.actualStateOfWorld)
                    ...
			} else {
				volumeToAttach := operationexecutor.VolumeToAttach{
					VolumeName: volumeToMount.VolumeName,
					VolumeSpec: volumeToMount.VolumeSpec,
					NodeName:   rc.hostName,
				}
				err := rc.operationExecutor.AttachVolume(volumeToAttach, rc.actualStateOfWorld)
			}
		} else if !volMounted || cache.IsRemountRequiredError(err) {
			err := rc.operationExecutor.MountVolume(
				rc.waitForAttachTimeout,
				volumeToMount.VolumeToMount,
				rc.actualStateOfWorld)
		}
	}

	for _, attachedVolume := range rc.actualStateOfWorld.GetUnmountedVolumes() {
		if !rc.desiredStateOfWorld.VolumeExists(attachedVolume.VolumeName) {
			if attachedVolume.GloballyMounted {
				err := rc.operationExecutor.UnmountDevice(
					attachedVolume.AttachedVolume, rc.actualStateOfWorld)
			} else {
				if rc.controllerAttachDetachEnabled || !attachedVolume.PluginIsAttachable {
					rc.actualStateOfWorld.MarkVolumeAsDetached(
						attachedVolume.VolumeName, rc.hostName)
				} else {
					err := rc.operationExecutor.DetachVolume(
						attachedVolume.AttachedVolume, false /* verifySafeToDetach */, rc.actualStateOfWorld)
				}
			}
		}
	}
}
```

reconcile首先从actualStateOfWorld获取已经挂载的volume信息，然后查看该volume是否存在于desiredStateOfWorld,假如不存在就卸载。

接着从desiredStateOfWorld获取需要挂载的volumes。与actualStateOfWorld比较，假如没有挂载，则进行挂载。

reconciler.Run还会调用rc.reconstruct()定时去重构，这里我们就不展开了。

## 二、statusManager

statusManager的作用比较简单，就是与kube-apiserver进行通信，定期更新Pod的Status。

```
func (m *manager) Start() {
	if m.kubeClient == nil {
		glog.Infof("Kubernetes client is nil, not starting status manager.")
		return
	}

	glog.Info("Starting to sync pod status with apiserver")
	syncTicker := time.Tick(syncPeriod)

	go wait.Forever(func() {
		select {
		case syncRequest := <-m.podStatusChannel:
			m.syncPod(syncRequest.podUID, syncRequest.status)
		case <-syncTicker:
			m.syncBatch()
		}
	}, 0)
}
```

函数启动一个go routine去执行任务。从podStatusChannel读取信息，然后更新Pod。由于statusManager有一个podStatuses      map[types.UID]versionedPodStatus的成员变量，本身缓存了pod的status信息，因此会定期syncBatch。

谁会往podStatusChannel写入信息呢？答案是SetPodStatus函数，在Pod的生命周期管理中，会发生Pod的状态的各种变化，就是通过SetPodStatus去写入状态的，然后再有statusManager更新到kube-apiserver，写入etcd中。

## 三、probeManager

probeManager主要用来管理探针的。存在两种probe，一种是readiness，另外一种是liveness


### 1、readinessProbe和livenessProbe

* **readinessProbe**：检测容器是否可以接受请求，假如failed。endpoint controller将移除从service的endpoints中移除该pod。readiness的初始值为failure。假如一个容器没有定义readinessProbe，则默认它的值为Success

* **livenessProbe**：检测容器是否存活，假如探针失败，则kubelet会杀死这个container。后续的步骤取决于RestartPolicy的值。liveness的初始值为Succses。假如一个容器没有定义livenessProbe，则默认它的值为Success



### 2、probeManager.AddPod

当创建容器时，最后一步通常都是probeManager.AddPod，我们来看看这一步做了什么。

**kubernetes/pkg/kubelet/prober/prober_manager.go +130**

```go
func (m *manager) AddPod(pod *api.Pod) {
	m.workerLock.Lock()
	defer m.workerLock.Unlock()

	key := probeKey{podUID: pod.UID}
	for _, c := range pod.Spec.Containers {
		key.containerName = c.Name

		if c.ReadinessProbe != nil {
			key.probeType = readiness
			if _, ok := m.workers[key]; ok {
				glog.Errorf("Readiness probe already exists! %v - %v",
					format.Pod(pod), c.Name)
				return
			}
			w := newWorker(m, readiness, pod, c)
			m.workers[key] = w
			go w.run()
		}

		if c.LivenessProbe != nil {
			key.probeType = liveness
			if _, ok := m.workers[key]; ok {
				glog.Errorf("Liveness probe already exists! %v - %v",
					format.Pod(pod), c.Name)
				return
			}
			w := newWorker(m, liveness, pod, c)
			m.workers[key] = w
			go w.run()
		}
	}
}

```

该函数会扫描该Pod的所有containers，检查他们是否有readinessProbe或者livenessProbe。假如有的话，创建一个woker，然后启动一个go routine运行该worker

**kubernetes/pkg/kubelet/prober/worker.go +99**

```go
func (w *worker) run() {
	probeTickerPeriod := time.Duration(w.spec.PeriodSeconds) * time.Second
	probeTicker := time.NewTicker(probeTickerPeriod)

	defer func() {
		probeTicker.Stop()
		if !w.containerID.IsEmpty() {
			w.resultsManager.Remove(w.containerID)
		}

		w.probeManager.removeWorker(w.pod.UID, w.container.Name, w.probeType)
	}()

	time.Sleep(time.Duration(rand.Float64() * float64(probeTickerPeriod)))

probeLoop:
	for w.doProbe() {
		// Wait for next probe tick.
		select {
		case <-w.stopCh:
			break probeLoop
		case <-probeTicker.C:
			// continue
		}
	}
}

func (w *worker) doProbe() (keepGoing bool) {
	defer runtime.HandleCrash(func(_ interface{}) { keepGoing = true })

	status, ok := w.probeManager.statusManager.GetPodStatus(w.pod.UID)
	if !ok {
		// Either the pod has not been created yet, or it was already deleted.
		glog.V(3).Infof("No status for pod: %v", format.Pod(w.pod))
		return true
	}

	// Worker should terminate if pod is terminated.
	if status.Phase == api.PodFailed || status.Phase == api.PodSucceeded {
		glog.V(3).Infof("Pod %v %v, exiting probe worker",
			format.Pod(w.pod), status.Phase)
		return false
	}

	c, ok := api.GetContainerStatus(status.ContainerStatuses, w.container.Name)
	if !ok || len(c.ContainerID) == 0 {
		// Either the container has not been created yet, or it was deleted.
		glog.V(3).Infof("Probe target container not found: %v - %v",
			format.Pod(w.pod), w.container.Name)
		return true // Wait for more information.
	}

	if w.containerID.String() != c.ContainerID {
		if !w.containerID.IsEmpty() {
			w.resultsManager.Remove(w.containerID)
		}
		w.containerID = kubecontainer.ParseContainerID(c.ContainerID)
		w.resultsManager.Set(w.containerID, w.initialValue, w.pod)
		// We've got a new container; resume probing.
		w.onHold = false
	}

	if w.onHold {
		// Worker is on hold until there is a new container.
		return true
	}

	if c.State.Running == nil {
		glog.V(3).Infof("Non-running container probed: %v - %v",
			format.Pod(w.pod), w.container.Name)
		if !w.containerID.IsEmpty() {
			w.resultsManager.Set(w.containerID, results.Failure, w.pod)
		}
		// Abort if the container will not be restarted.
		return c.State.Terminated == nil ||
			w.pod.Spec.RestartPolicy != api.RestartPolicyNever
	}

	if int32(time.Since(c.State.Running.StartedAt.Time).Seconds()) < w.spec.InitialDelaySeconds {
		return true
	}

	result, err := w.probeManager.prober.probe(w.probeType, w.pod, status, w.container, w.containerID)
	if err != nil {
		// Prober error, throw away the result.
		return true
	}

	if w.lastResult == result {
		w.resultRun++
	} else {
		w.lastResult = result
		w.resultRun = 1
	}

	if (result == results.Failure && w.resultRun < int(w.spec.FailureThreshold)) ||
		(result == results.Success && w.resultRun < int(w.spec.SuccessThreshold)) {
		return true
	}

	w.resultsManager.Set(w.containerID, result, w.pod)

	if w.probeType == liveness && result == results.Failure {
		w.onHold = true
	}

	return true
}
```

### 3、w.resultsManager.Set(w.containerID, result, w.pod)

Set函数写将probe的结果写入到updates这个channel中

**kubernetes/pkg/kubelet/prober/results/results_manager.go**

```
func (m *manager) Set(id kubecontainer.ContainerID, result Result, pod *api.Pod) {
	if m.setInternal(id, result) {
		m.updates <- Update{id, result, pod.UID}
	}
}
```

### 4、updates的消费者

updates的消费者其实位于syncLoopIteration函数中。当probe为Failure时，调用handler.HandlePodSyncs更新Pod，将删除原有的container，创建新的container。

```go
	case update := <-kl.livenessManager.Updates():
		if update.Result == proberesults.Failure {
			pod, ok := kl.podManager.GetPodByUID(update.PodUID)
			if !ok {
				// If the pod no longer exists, ignore the update.
				glog.V(4).Infof("SyncLoop (container unhealthy): ignore irrelevant update: %#v", update)
				break
			}
			glog.V(1).Infof("SyncLoop (container unhealthy): %q", format.Pod(pod))
			handler.HandlePodSyncs([]*api.Pod{pod})
		}
```

### 5、kl.probeManager.Start()

**kubernetes/pkg/kubelet/prober/prober_manager.go +98**

```go
func (m *manager) Start() {
	// Start syncing readiness.
	go wait.Forever(m.updateReadiness, 0)
}
...
func (m *manager) updateReadiness() {
	update := <-m.readinessManager.Updates()

	ready := update.Result == results.Success
	m.statusManager.SetContainerReadiness(update.PodUID, update.ContainerID, ready)
}
```

readiness的处理。

## 四、pleg
pleg，即PodLifecycleEventGenerator，pleg会记录Pod生命周期中的各种事件，如容器的启动、终止等，这些事件会写入etcd中，同时他检测到container异常退出时，他会通知kubelet，然后重启创建该container

### 1、Start()函数

**kubernetes/pkg/kubelet/pleg/generic.go +120**

```go
func (g *GenericPLEG) Start() {
	go wait.Until(g.relist, g.relistPeriod, wait.NeverStop)
}
```

每隔1s会执行一次relist

**kubernetes/pkg/kubelet/pleg/generic.go +180**

```
func (g *GenericPLEG) relist() {
    ...
	podList, err := g.runtime.GetPods(true)
	pods := kubecontainer.Pods(podList)
	g.podRecords.setCurrent(pods)

	eventsByPodID := map[types.UID][]*PodLifecycleEvent{}
	for pid := range g.podRecords {
		oldPod := g.podRecords.getOld(pid)
		pod := g.podRecords.getCurrent(pid)
		// Get all containers in the old and the new pod.
		allContainers := getContainersFromPods(oldPod, pod)
		for _, container := range allContainers {
			e := computeEvent(oldPod, pod, &container.ID)
			updateEvents(eventsByPodID, e)
		}
	}

	var needsReinspection map[types.UID]*kubecontainer.Pod
	if g.cacheEnabled() {
		needsReinspection = make(map[types.UID]*kubecontainer.Pod)
	}

	// If there are events associated with a pod, we should update the
	// podCache.
	for pid, events := range eventsByPodID {
		pod := g.podRecords.getCurrent(pid)
		if g.cacheEnabled() {
			if err := g.updateCache(pod, pid); err != nil {
                ...
				continue
			} else if _, found := g.podsToReinspect[pid]; found {
				delete(g.podsToReinspect, pid)
			}
		}
		// Update the internal storage and send out the events.
		g.podRecords.update(pid)
		for i := range events {
			if events[i].Type == ContainerChanged || events[i].Type == ContainerRemoved {
				continue
			}
			g.eventChannel <- events[i]
		}
	}

	if g.cacheEnabled() {
		// reinspect any pods that failed inspection during the previous relist
		if len(g.podsToReinspect) > 0 {
			glog.V(5).Infof("GenericPLEG: Reinspecting pods that previously failed inspection")
			for pid, pod := range g.podsToReinspect {
				if err := g.updateCache(pod, pid); err != nil {
					glog.Errorf("PLEG: pod %s/%s failed reinspection: %v", pod.Name, pod.Namespace, err)
					needsReinspection[pid] = pod
				}
			}
		}
		g.cache.UpdateTime(timestamp)
	}

	g.podsToReinspect = needsReinspection
}
```

relist首先调用g.runtime.GetPods(true)获取所有的pods，进而生成podRecords，然后检查所有的podRecords。主要是利用g.podRecords.getOld和g.podRecords.getCurrent获取这一轮和上一轮的该Pod信息，接着检查该Pod的所有containers，从而检查出容器启动、退出等事件，即computeEvent(oldPod, pod, &container.ID)生成事件，updateEvents(eventsByPodID, e)则将事件更新到eventsByPodID这个map中。

接下来遍历eventsByPodID，遍历的目的主要是将event记录到etcd中，同时将事件发送至g.eventChannel中。即当 events[i].Type == ContainerChanged或events[i].Type == ContainerRemoved时，不发送，否则发送到eventChannel中

### 2、eventChannel的消费者

eventChannel的消费者其实位于syncLoopIteration函数中，syncLoopIteration传入了一个参数plegCh。plegCh := kl.pleg.Watch()返回的正式eventChannel。

syncLoopIteration将从这个channel中读取pod的生命周期事件，进行处理

**kubernetes/pkg/kubelet/kubelet.go +2593**

```
	case e := <-plegCh:
		// PLEG event for a pod; sync it.
		pod, ok := kl.podManager.GetPodByUID(e.ID)
		if !ok {
			// If the pod no longer exists, ignore the event.
			glog.V(4).Infof("SyncLoop (PLEG): ignore irrelevant event: %#v", e)
			break
		}
		glog.V(2).Infof("SyncLoop (PLEG): %q, event: %#v", format.Pod(pod), e)
		handler.HandlePodSyncs([]*api.Pod{pod})
```

handler.HandlePodSyncs函数最终还是调用dispatchWork去处理Pod。当container启动失败时，则会重新创建该container。

**kubernetes/pkg/kubelet/kubelet.go +2758**

```
func (kl *Kubelet) HandlePodSyncs(pods []*api.Pod) {
	start := kl.clock.Now()
	for _, pod := range pods {
		mirrorPod, _ := kl.podManager.GetMirrorPodByPod(pod)
		kl.dispatchWork(pod, kubetypes.SyncPodSync, mirrorPod, start)
	}
}
```

## 五、GC

GC，Garbage Collection。kubelet会启动两个GC，分别回收container和image。其中container的回收频率为1分钟一次，而image回收频率为5分钟一次。

**kubernetes/pkg/kubelet/kubelet.go +945**

```go
func (kl *Kubelet) StartGarbageCollection() {
	go wait.Until(func() {
		if err := kl.containerGC.GarbageCollect(kl.sourcesReady.AllReady()); err != nil {
			glog.Errorf("Container garbage collection failed: %v", err)
		}
	}, ContainerGCPeriod, wait.NeverStop)

	go wait.Until(func() {
		if err := kl.imageManager.GarbageCollect(); err != nil {
			glog.Errorf("Image garbage collection failed: %v", err)
		}
	}, ImageGCPeriod, wait.NeverStop)
}
```

### 1、container gc

**kubernetes/pkg/kubelet/container/container_gc.go +66**

```go
func (cgc *realContainerGC) GarbageCollect(allSourcesReady bool) error {
	return cgc.runtime.GarbageCollect(cgc.policy, allSourcesReady)
}
```

**kubernetes/pkg/kubelet/dockertools/docker_manager.go +2351**

```go
func (dm *DockerManager) GarbageCollect(gcPolicy kubecontainer.ContainerGCPolicy, allSourcesReady bool) error {
	return dm.containerGC.GarbageCollect(gcPolicy, allSourcesReady)
}
```

**kubernetes/pkg/kubelet/dockertools/container_gc.go**

```go
func (cgc *containerGC) GarbageCollect(gcPolicy kubecontainer.ContainerGCPolicy, allSourcesReady bool) error {
	// Separate containers by evict units.
	evictUnits, unidentifiedContainers, err := cgc.evictableContainers(gcPolicy.MinAge)
	if err != nil {
		return err
	}

	// Remove unidentified containers.
	for _, container := range unidentifiedContainers {
		glog.Infof("Removing unidentified dead container %q with ID %q", container.name, container.id)
		err = cgc.client.RemoveContainer(container.id, dockertypes.ContainerRemoveOptions{RemoveVolumes: true})
		if err != nil {
			glog.Warningf("Failed to remove unidentified dead container %q: %v", container.name, err)
		}
	}

	// Remove deleted pod containers if all sources are ready.
	if allSourcesReady {
		for key, unit := range evictUnits {
			if cgc.isPodDeleted(key.uid) {
				cgc.removeOldestN(unit, len(unit)) // Remove all.
				delete(evictUnits, key)
			}
		}
	}

	// Enforce max containers per evict unit.
	if gcPolicy.MaxPerPodContainer >= 0 {
		cgc.enforceMaxContainersPerEvictUnit(evictUnits, gcPolicy.MaxPerPodContainer)
	}

	// Enforce max total number of containers.
	if gcPolicy.MaxContainers >= 0 && evictUnits.NumContainers() > gcPolicy.MaxContainers {
		// Leave an equal number of containers per evict unit (min: 1).
		numContainersPerEvictUnit := gcPolicy.MaxContainers / evictUnits.NumEvictUnits()
		if numContainersPerEvictUnit < 1 {
			numContainersPerEvictUnit = 1
		}
		cgc.enforceMaxContainersPerEvictUnit(evictUnits, numContainersPerEvictUnit)

		// If we still need to evict, evict oldest first.
		numContainers := evictUnits.NumContainers()
		if numContainers > gcPolicy.MaxContainers {
			flattened := make([]containerGCInfo, 0, numContainers)
			for uid := range evictUnits {
				flattened = append(flattened, evictUnits[uid]...)
			}
			sort.Sort(byCreated(flattened))

			cgc.removeOldestN(flattened, numContainers-gcPolicy.MaxContainers)
		}
	}

	// Remove dead symlinks - should only happen on upgrade
	// from a k8s version without proper log symlink cleanup
	logSymlinks, _ := filepath.Glob(path.Join(cgc.containerLogsDir, fmt.Sprintf("*.%s", LogSuffix)))
	for _, logSymlink := range logSymlinks {
		if _, err = os.Stat(logSymlink); os.IsNotExist(err) {
			err = os.Remove(logSymlink)
			if err != nil {
				glog.Warningf("Failed to remove container log dead symlink %q: %v", logSymlink, err)
			}
		}
	}

	return nil
}
```

gcPolicy是gc的策略，MinAge表示被回收的container的最小年龄，MaxPerPodContainer每个Pod最多允许的dead container数目，而MaxContainers表示节点最多允许的dead containers。

```
type ContainerGCPolicy struct {
	// Minimum age at which a container can be garbage collected, zero for no limit.
	MinAge time.Duration

	// Max number of dead containers any single pod (UID, container name) pair is
	// allowed to have, less than zero for no limit.
	MaxPerPodContainer int

	// Max number of total dead containers, less than zero for no limit.
	MaxContainers int
}
```

evictableContainers主要是对containers进行过滤，过滤掉正在运行的container或年龄小于MinAge的container。

首先删除unidentifiedContainers。接着删除那些Pod已被删除的containers。然后处理那些Pod还存在，但还存在dead containers，每个Pod最多允许存在MaxPerPodContainer个dead container。

接着检查剩余的evictUnits.NumContainers，假如大于MaxContainers，则继续删除。每个EvictUnits删除gcPolicy.MaxContainers / evictUnits.NumEvictUnits()个dead containers。假如还是不符合要求，则删除numContainers-gcPolicy.MaxContainers个container，确保最后最多只剩MaxContainers个dead containers。

最后则回收/var/log/containers下已经被删除的容器的日志文件。


### 2、image gc

**kubernetes/pkg/kubelet/image_manager.go +210**

```go
func (im *realImageManager) GarbageCollect() error {
	// Get disk usage on disk holding images.
	fsInfo, err := im.cadvisor.ImagesFsInfo()
	if err != nil {
		return err
	}
	capacity := int64(fsInfo.Capacity)
	available := int64(fsInfo.Available)
	if available > capacity {
		glog.Warningf("available %d is larger than capacity %d", available, capacity)
		available = capacity
	}

	// Check valid capacity.
	if capacity == 0 {
		err := fmt.Errorf("invalid capacity %d on device %q at mount point %q", capacity, fsInfo.Device, fsInfo.Mountpoint)
		im.recorder.Eventf(im.nodeRef, api.EventTypeWarning, container.InvalidDiskCapacity, err.Error())
		return err
	}

	// If over the max threshold, free enough to place us at the lower threshold.
	usagePercent := 100 - int(available*100/capacity)
	if usagePercent >= im.policy.HighThresholdPercent {
		amountToFree := capacity*int64(100-im.policy.LowThresholdPercent)/100 - available
		glog.Infof("[ImageManager]: Disk usage on %q (%s) is at %d%% which is over the high threshold (%d%%). Trying to free %d bytes", fsInfo.Device, fsInfo.Mountpoint, usagePercent, im.policy.HighThresholdPercent, amountToFree)
		freed, err := im.freeSpace(amountToFree, time.Now())
		if err != nil {
			return err
		}

		if freed < amountToFree {
			err := fmt.Errorf("failed to garbage collect required amount of images. Wanted to free %d, but freed %d", amountToFree, freed)
			im.recorder.Eventf(im.nodeRef, api.EventTypeWarning, container.FreeDiskSpaceFailed, err.Error())
			return err
		}
	}

	return nil
}
```

image gc也有自己的gc policy

```
type ImageGCPolicy struct {
	// Any usage above this threshold will always trigger garbage collection.
	// This is the highest usage we will allow.
	HighThresholdPercent int

	// Any usage below this threshold will never trigger garbage collection.
	// This is the lowest threshold we will try to garbage collect to.
	LowThresholdPercent int

	// Minimum age at which a image can be garbage collected.
	MinAge time.Duration
}
```

高于HighThresholdPercent将触发回收，LowThresholdPercent表示经过回收后的最低比例，MinAge表示被回收的image的最小年龄。

回收函数是m.freeSpace(amountToFree, time.Now())

**kubernetes/pkg/kubelet/image_manager.go +256**

```go
func (im *realImageManager) freeSpace(bytesToFree int64, freeTime time.Time) (int64, error) {
	err := im.detectImages(freeTime)

	im.imageRecordsLock.Lock()
	defer im.imageRecordsLock.Unlock()

	// Get all images in eviction order.
	images := make([]evictionInfo, 0, len(im.imageRecords))
	for image, record := range im.imageRecords {
		images = append(images, evictionInfo{
			id:          image,
			imageRecord: *record,
		})
	}
	sort.Sort(byLastUsedAndDetected(images))

	// Delete unused images until we've freed up enough space.
	var lastErr error
	spaceFreed := int64(0)
	for _, image := range images {
		glog.V(5).Infof("Evaluating image ID %s for possible garbage collection", image.id)
		// Images that are currently in used were given a newer lastUsed.
		if image.lastUsed.Equal(freeTime) || image.lastUsed.After(freeTime) {
			glog.V(5).Infof("Image ID %s has lastUsed=%v which is >= freeTime=%v, not eligible for garbage collection", image.id, image.lastUsed, freeTime)
			break
		}

		// Avoid garbage collect the image if the image is not old enough.
		// In such a case, the image may have just been pulled down, and will be used by a container right away.

		if freeTime.Sub(image.firstDetected) < im.policy.MinAge {
			glog.V(5).Infof("Image ID %s has age %v which is less than the policy's minAge of %v, not eligible for garbage collection", image.id, freeTime.Sub(image.firstDetected), im.policy.MinAge)
			continue
		}

		// Remove image. Continue despite errors.
		glog.Infof("[ImageManager]: Removing image %q to free %d bytes", image.id, image.size)
		err := im.runtime.RemoveImage(container.ImageSpec{Image: image.id})
		if err != nil {
			lastErr = err
			continue
		}
		delete(im.imageRecords, image.id)
		spaceFreed += image.size

		if spaceFreed >= bytesToFree {
			break
		}
	}

	return spaceFreed, lastErr
}
```

detectImages会将正在使用的image的lastUsed的时间设置为现在，因此回收时会忽略。此外镜像的年龄小于policy.MinAge的镜像也会忽略。接着回收剩余的镜像，一旦回收的容量大于bytesToFree则跳出循环，返回回收的容量。
